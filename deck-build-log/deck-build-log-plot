#!/usr/bin/env python2

from __future__ import print_function

import argparse
import base64
import collections
import datetime
import io
import json
import os
import re
import subprocess
import urllib
import urllib2

import matplotlib.cm
import matplotlib.dates
import matplotlib.patches
import matplotlib.pyplot


# https://bugzilla.readthedocs.io/en/latest/api/core/v1/attachment.html#create-attachment
bugzilla_template = 'https://bugzilla.redhat.com/rest/bug/{}/attachment'


parser = argparse.ArgumentParser(
    description='Plot matching build failures over time.',
)
parser.add_argument(
    '-b', '--bug', dest='bug', type=int,
    help='Red Hat Bugzilla bug number for uploading an attachment.',
)
parser.add_argument(
    '-k', '--api-key', dest='api_key',
    help='Red Hat Bugzilla API key ( https://bugzilla.readthedocs.io/en/latest/using/preferences.html#api-keys ).',
)
parser.add_argument(
    '-r', '--repo', dest='repo',
    help='Path to local Git repository to overlay on the graph (PATH@REF).'
)
parser.add_argument(
    '-l', '--min-duration', dest='min_duration', default=0, type=int,
    help='Minimum duration of a job in seconds.  If unset, defaults to 0s.'
)
parser.add_argument(
    '-u', '--max-duration', dest='max_duration', type=int,
    help='Maximum duraftion of a job in seconds.  If unset, defaults to unlimited.'
)
parser.add_argument(
    'regexps', metavar='REGEXP', nargs='+',
    help='Python regular expression for matching build logs.',
)
args = parser.parse_args()

regexps = [re.compile(regexp) for regexp in args.regexps]
if args.bug is not None:
    if args.api_key is None:
        raise ValueError('if --bug is set, --api-key must also be set')

    if len(regexps) != 1:
        raise ValueError('bug submission requires a single REGEXP')


def start_stop(job):
    start = datetime.datetime.utcfromtimestamp(int(job['started']))
    stop = None
    if job.get('finished', ''):
        stop = datetime.datetime.strptime(job['finished'], '%Y-%m-%dT%H:%M:%SZ')  # fromisoformat is new in Python 3.7
    return (start, stop)


def pull_uri(job):
    refs = job.get('refs', {})
    pull = refs.get('pulls', [{}])[0]
    link = pull.get('link', None)
    if link:
        return link
    number = pull.get('number', None)
    if number:
        return 'https://github.com/{}/{}/pull/{}'.format(
            refs['org'], refs['repo'], number)
    return job['url']


def value(job, build_log=None):
    # FIXME: calculate your y-axis value
    start, stop = start_stop(job=job)
    return (stop - start).total_seconds() / 60.


color_map = matplotlib.cm.get_cmap('Set1')
no_match_color = [0, 0, 1]
success_color = [0, 1, 0, 0.3]
def color(job, build_log=None, links=None):
    # FIXME: calculate your dot color, or return None to exclude the job
    if build_log is None:
        if job['state'] == 'triggered':
            return None  # I don't care about these yet
        if job['state'] == 'aborted':
            return None  # I don't care about these yet
        if job['state'] == 'pending':
            return None  # no duration for value()
        if job['state'] == 'failure':
            return None  # non e2e-aws job, so we didn't pull its build log
        if job['state'] == 'success':
            return success_color
        raise ValueError(job)
    for i, regexp in enumerate(regexps):
        match = regexp.search(build_log)
        if match:
            if links is not None:
                links[match.group(0)][pull_uri(job=job)].append(job)
            return color_map(float(i) / len(regexps))
    return no_match_color


cache = os.path.join(os.path.expanduser('~'), '.cache', 'openshift-deck-build-logs')
entries = []
links = collections.defaultdict(lambda: collections.defaultdict(list))

known_urls = set()
namespace_regexp = re.compile('^.* Using namespace (.*)$')
for root, _, files in os.walk(cache):
    if 'job.json' in files and 'build-log.txt' in files:
        with open(os.path.join(root, 'job.json'), 'rb') as f:
            job = json.load(f)
        with open(os.path.join(root, 'build-log.txt'), 'r') as f:
            build_log = f.read()
        #try:
        #    with open(os.path.join(root, 'pods.json'), 'r') as f:
        #        pods = f.read()
        #except Exception:
        #    pods = ''
        known_urls.add(job['url'])
        for line in build_log.splitlines():
            match = namespace_regexp.match(line)
            if match:
                job['namespace'] = match.group(1)
                break
        #job_color = color(job=job, build_log=build_log + pods, links=links)
        job_color = color(job=job, build_log=build_log, links=links)
        if job_color is None:
            continue
        start, stop = start_stop(job=job)
        duration = (stop - start).total_seconds()
        if duration < args.min_duration or (args.max_duration is not None and duration > args.max_duration):
            continue
        entries.append({
            'started': start,
            'value': value(job=job, build_log=build_log),
            'color': job_color,
            'url': job['url'],
        })

now = None
with open(os.path.join(cache, 'jobs.json'), 'rb') as f:
    for job in json.load(f):
        start, stop = start_stop(job=job)
        if now is None or start > now:
            now = start
        if stop is not None and stop > now:
            now = stop
        if job['url'] in known_urls:
            continue
        if stop is not None:
            duration = (stop - start).total_seconds()
            if duration < args.min_duration or (args.max_duration is not None and duration > args.max_duration):
                continue
        job_color = color(job=job, links=links)
        if job_color is None:
            continue
        entries.append({
            'started': start,
            'value': value(job=job),
            'color': job_color,
            'url': job['url'],
        })


pull_lines = []
for total, match, pulls in sorted(
        (
            (
                sum(len(jobs) for jobs in pulls.values()),
                match,
                pulls,
            )
            for (match, pulls) in links.items()
        ), reverse=True):
    pull_lines.append('{}\t{}'.format(total, match))
    for count, link, jobs in sorted(
            (
                (
                    len(jobs),
                    link,
                    jobs,
                )
                for link, jobs in pulls.items()
            ), reverse=True):
        jobs.sort(key=lambda job: int(job['started']))
        namespaces = [job['namespace'] for job in jobs if job.get('namespace')]
        if namespaces:
            latest_namespace = namespaces[-1]
        else:
            latest_namespace = 'NO NAMESPACE'
        pull_lines.append('\t{}\t{}\t{}'.format(count, link, latest_namespace))
for line in pull_lines:
    print(line)


entries.sort(key=lambda entry: entry['started'])
entries = [  # example of date filtering
    entry
    for entry in entries
    if entry['started'] > (datetime.datetime.now() - datetime.timedelta(days=1))
]

# FIXME: adjust to match color()
no_match_failures = len([e for e in entries if e['color'] == no_match_color])
successes = len([e for e in entries if e['color'] == success_color])
failures = len(entries) - successes
match_failures = []
legend_entries = []
for i, regexp in enumerate(regexps):
    match_color = color_map(float(i) / len(regexps))
    count = len([e for e in entries if e['color'] == match_color])
    match_failures.append(count)
    legend_entries.append((
        match_color,
        '{} ({}% of all failures) {!r}'.format(
            count,
            count * 100 // failures,
            regexp.pattern,
        ),
    ))
legend_entries.extend([
    (
        no_match_color,
        '{} ({}% of all failures) other failures'.format(
            no_match_failures,
            no_match_failures * 100 // failures,
        ),
    ),
    (
        success_color,
        '{} ({}% of jobs) success'.format(
            successes,
            successes * 100 // (failures + successes),
        ),
    ),
])

legend_handles = []
for legend_color, label in legend_entries:
    legend_handles.append(matplotlib.patches.Circle(
        xy=(0.5, 0.5),  # Matplotlib v1.2, the ancient beast, seems to just draw a patch despite us calling for a Circle, but whatever
        radius=0.25,
        color=legend_color[:3],  # slice off the alpha channel
        label=label,
    ))

start = entries[0]['started']
stop = entries[-1]['started']

commits = []
if args.repo:
    if '@' in args.repo:
        repo, ref = args.repo.split('@', 1)
    else:
        repo = args.repo
        ref = 'origin/master'
    subprocess.check_call(
        args=['git', 'fetch', 'origin'],
        close_fds=True,
        cwd=repo,
    )
    environ = os.environ.copy()
    environ['TZ'] = 'UTC'
    environ['LANG'] = 'en_US.UTF-8'
    log = subprocess.check_output(
        args=['git', 'log', '--first-parent', '--format=%ct %H %s', '-z', '--since={}'.format(start.isoformat()), ref],
        close_fds=True,
        cwd=repo,
        env=environ,
    ).decode('utf-8')
    for line in log.split('\0')[:-1]:
        timestamp, commit, subject = line.split(' ', 2)
        commits.append({
            'timestamp': datetime.datetime.utcfromtimestamp(float(timestamp)),
            'hash': commit,
            'subject': subject,
        })

figure = matplotlib.pyplot.figure()
figure.set_size_inches(20, 5)
axes = figure.add_subplot(1, 1, 1)
axes.set_title('recent e2e-aws builds')  # FIXME: adjust to match value() and color()
axes.set_ylabel('job duration (minutes)')  # FIXME: adjust to match value()
axes.set_xlabel('start time ({} to {} UTC)'.format(start.isoformat(), stop.isoformat()))

max_value = max(entry['value'] for entry in entries)
# most recent possible job results
axes.plot(
    [now - datetime.timedelta(minutes=max_value), now],  # FIXME: adjust to match value()
    [max_value, 0],
    color='black',
    alpha=0.25,
    linestyle='-',
)

scatter = axes.scatter(
    x=[entry['started'] for entry in entries],
    y=[entry['value'] for entry in entries],
    c=[entry['color'] for entry in entries],
    edgecolor='',
)
scatter.set_urls([entry['url'] for entry in entries])

if commits:
    print()
    for commit in commits:
        print('{} {} {}'.format(
            commit['timestamp'].isoformat() + 'Z',
            commit['hash'][:10],
            commit['subject'],
        ))
    scatter = axes.scatter(
        x=[c['timestamp'] for c in commits],
        y=[max_value]*len(commits),
        s=30,
        c='none',
        marker='d',
        linewidths=3,
        edgecolors='black',
    )
    scatter.set_urls(['subject:{}'.format(urllib.quote(c['subject'])) for c in commits])

locator = matplotlib.dates.HourLocator(interval=4)
axes.xaxis.set_major_locator(locator)
axes.xaxis.set_major_formatter(matplotlib.dates.AutoDateFormatter(locator))
axes.set_xlim(start, stop)
axes.set_ylim(min(e['value'] for e in entries), max_value)
axes.legend(
    legend_handles,
    # Matplotlib's arg parsing is crazy, and requires us to set to positional args to get the first treated as handles
    [h.get_label() for h in legend_handles],
    loc='upper left',
    frameon=False,
)
figure.tight_layout()
figure.autofmt_xdate()
figure.savefig('deck-build-log.svg')
figure.savefig('deck-build-log.png')

if args.bug is not None:
    url = bugzilla_template.format(args.bug) + '?api_key={}'.format(args.api_key)
    buf = io.BytesIO()
    figure.savefig(buf, format='svg')
    if len(pull_lines) > 10:
        pull_lines = pull_lines[:10] + ['...']
    data = {
        'ids': [args.bug],
        'data': base64.b64encode(buf.getvalue()),
        'file_name': 'deck-build-log.svg',
        'summary': 'Occurrences of this error in CI from {} to {} UTC'.format(
            start.strftime('%Y-%m-%dT%H:%M'),
            stop.strftime('%Y-%m-%dT%H:%M'),
        ),
        'content_type': 'image/svg+xml',
        'comment': '''This occurred in {matches} of our {total} failures ({percent}%) in *-e2e-aws* jobs across the whole CI system over the past {hours} hours.  Generated with [1]:

  $ deck-build-log-plot {regexp!r}
  {lines}

[1]: https://github.com/wking/openshift-release/tree/debug-scripts/deck-build-log
'''.format(
            matches=match_failures[0],
            total=failures,
            percent=match_failures[0] * 100 // failures,
            hours=int((stop - start).total_seconds()) // 3600,
            regexp=regexps[0].pattern,
            lines='\n  '.join(pull_lines),
        ),
    }
    try:
        response = urllib2.urlopen(urllib2.Request(
            url=url,
            data=json.dumps(data),
            headers={
                'Accept': 'application/json',
                'Content-Type': 'application/json',
                'X-BUGZILLA-API-KEY': args.api_key,  # but not supported yet by our Bugzilla 5.0.4.rh14, hence the query param above
            },
        ))
    except urllib2.HTTPError as error:
        response = error
    print('POSTed attachment to bug {} returned {}'.format(args.bug, response.getcode()))
    print(response.read())
    response.close()
